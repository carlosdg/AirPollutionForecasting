## Introducción

El manejo y análisis de datos supone afrontar grandes retos pero, a cambio, puede ofrecer una valiosa recompensa: **información**. Algunas empresas, a partir de datos sobre la interacción de sus usuarios pueden extraer qué les gusta, lo que supone una información muy útil. Pueden usarla, por ejemplo, para mostrar anuncios personalizados, para recomendar productos de la misma empresa que les pueda gustar o para crear un nuevo producto que se adapte a los gustos del mayor número de usuarios posibles. En el ámbito de la investigación, teniendo datos sobre tratamientos a pacientes se pueden buscar patrones que ayuden a los médicos a elegir un mejor tratamiento para futuros pacientes [1].

**La información permite tomar mejores decisiones al reducir incertidumbre** y su extracción a partir de datos también es de suma utilidad y un proceso habitual en el día a día de las personas. Por ejemplo, datos como qué másters sobre Ingeniería Informática existen, dónde habría que estudiar o qué se aprendería, serían de utilidad en el contexto de elegir un máster sobre Ingeniería Informática. Analizando estos datos se puede obtener información útil para la decisión como saber cuál es la opción más económica, saber en cuál se imparten asignaturas que gusten más al decisor, etc.

Desde la Antigüedad se ha hecho uso de la recogida de datos y extracción de información para la toma de decisiones. En la Antigua Roma se realizaban censos donde períodicamente se recogían datos de la población para decidir cuestiones como cuánto le correspondía pagar de impuestos a cada individuo y cuál era su clase en el servicio militar [2]. Con el desarrollo de las matemáticas y, en particular, de la estadística, se han ido creando y refinando técnicas para manejar y analizar datos. Sin embargo, toda la recogida y procesamiento tenía que ser llevada a cabo por personas, requiriendo mucho tiempo.

En 1890 se estimaba que el censo de Estados Unidos tomaría alrededor de 10 años en ser recogido y procesado. Para cuando se terminase tendrían que empezar el siguiente censo con una mayor población y llevando, por tanto, todavía más tiempo. Fue gracias a la máquina de Herman Hollerith, posterior fundador de IBM, que se pudo semi-automatizar el proceso para terminar el censo a tiempo [3]. El tiempo exacto que tomó varía según la fuente que se consulte, algunas hablan de unos pocos meses y otras de unos pocos años, pero, en general, gracias a la automatización se disminuyó notablemente el tiempo para procesar una gran cantidad de datos.

Aunque la capacidad de procesamiento de la máquina de Hollerith era muy limitado respecto a lo que podía hacer, en poco tiempo surgieron los computadores actuales, máquinas capaces de realizar cualquier cálculo. A principios del siglo XX David Hilbert presentó un conjunto de problemas matemáticos sin resolver que influenció parte de la investigación matemática del resto del siglo. En concreto el problema de la decisión (_Entscheidungsproblem_) [4] motivó varias investigaciones como las publicadas en 1936 por Alonzo Church y Alan Turing que demostraron, de manera independiente, que era imposible dar una solución general al problema. Para ello Turing definió lo que hoy se conoce como una máquina de Turing, un modelo matemático capaz de representar cualquier algoritmo, siendo la base sobre la que se constituye las Ciencias de la Computación.

A partir de entonces se tuvo el conocimiento necesario para crear máquinas físicas Turing completas. Es decir, máquinas computacionalmente equivalente a una Máquina de Turing Universal (salvo restricciones técnicas como la necesidad de una memoria infinita). Siendo una Máquina de Turing Universal una Máquina de Turing capaz de simular cualquier otra dada como entrada. Una desventaja de los primeros modelos de máquinas físicas es que tenían que ser recableadas para poder ejecutar una serie de cálculos distintos [5]. Sin embargo, gracias a la arquitectura de computadores de John von Neumann, se definió un modelo de máquina física capaz de almacenar en la memoria no sólo datos, sino programas (las series de cálculos a realizar).

Con el paso de un corto período de tiempo y con el foco puesto en hacer las computadoras cada vez más rápidas se lograron grandes avances tecnológicos: se consigió que las computadoras cada vez ocuparan un volumen menor, fueran capaces de realizar más operaciones por unidad de tiempo y costaran menos. Sin embargo, el acceso y almacenamiento de datos seguía suponiendo un desafío: empresas guardaban datos estructurados de sus clientes como, por ejemplo, tuplas de nombre, apellidos y lugar de residencia. Pero los programadores tenían que trabajar directamente con ficheros, los cuales no imponen ninguna estructura y entre ficheros puede haber cualquier relación lógica. Esto hacía que fuera fácil cometer errores y difícil saber la relación que había entre los datos. Por ello surgieron los modelos de base de datos y los sistemas gestores de bases de datos que, para mejorar esta situación, proveyeron una abstracción entre la estructura lógica de los datos y el almacenamiento de datos en ficheros.

A finales del siglo XX ya había surgido la familia de protocolos TCP/IP para conectar redes de ordenadores. Sin embargo, los investigadores todavía no disponían de una manera centralizada de compartir sus hallazgos a cualquier persona en el mundo que le pudiese interesar. Esto fue lo que motivó a Tim Berners-Lee a desarrollar los cimientos de la _World Wide Web_ [6].

## Referencias

- [1] Johns Hopkins Medicine. (2018, August 8). Using big data to predict
  immunotherapy responses. ScienceDaily. Retrieved November 25, 2018 from
  [www.sciencedaily.com/releases/2018/08/180808093858.htm](https://www.sciencedaily.com/releases/2018/08/180808093858.htm)

- [2] El reclutamiento. Universitat de València. Recogido el 26 de Noviembre de 2018 de [https://www.uv.es/alabau/gye/recluta.htm](https://www.uv.es/alabau/gye/recluta.htm)

- [3] Herman Hollerith. Columbia University. (Most recent update: Thu Oct 18 09:40:48 2018). Retrieved November 27, 2018 from [http://www.columbia.edu/cu/computinghistory/hollerith.html](http://www.columbia.edu/cu/computinghistory/hollerith.html)

- [4] Decision problem in a few sentences. David Joyce. (Most recent update: Feb 6, 2016). Retrieved November 29, 2018 from [https://www.quora.com/How-can-I-explain-Entscheidungs-problem-in-a-few-sentences-to-people-without-confusing-people/answer/David-Joyce-11](https://www.quora.com/How-can-I-explain-Entscheidungs-problem-in-a-few-sentences-to-people-without-confusing-people/answer/David-Joyce-11)

- [5] ENIAC. Paul A. Freiberger, Michael R. Swaine. (Most recent update: Oct 26, 2018). Retrieved November 29, 2018 from [https://www.britannica.com/technology/ENIAC](https://www.britannica.com/technology/ENIAC)

- [6] History of the web. Retrieved November 29, 2018 from [https://webfoundation.org/about/vision/history-of-the-web/](https://webfoundation.org/about/vision/history-of-the-web/)

## Fuentes Adicionales

- Big Data For Dummies, ISBN: 9781118644171, Marcia Kaufman, Dr. Fern Halper,
- [Datos vs Información](https://www.researchgate.net/post/What_is_the_difference_between_data_and_information/1)
- [Historia de la estadística](https://en.wikipedia.org/wiki/History_of_statistics)
  Alan Nugent, Judith Hurwitz
- [Breve historia del análisis de datos](https://www.flydata.com/blog/a-brief-history-of-data-analysis/)
- [Cómo hace uso Netflix del Big Data](https://medium.com/swlh/how-netflix-uses-big-data-20b5419c1edf)
- [El papel de los censores en la Antigua Roma](https://www.ancient.eu/censor/)
- [Breve explicación de cómo funcionaba la máquina de Hollerith](https://www.youtube.com/watch?v=9HXjLW7v-II)
- [Computer Fundamentals. Lecture 2. Dr Robert Harle. Cambridge University](https://www.cl.cam.ac.uk/teaching/1314/CompFund/Lecture2.pdf)
